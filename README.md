Overview

This project is a Markov chain–based text generator written in pure Python.
It learns statistical patterns from a large body of text and generates new text that resembles the style of the original input.

For demonstration purposes, the training data consists of a collection of Kanye West lyrics.
The generator produces new, original lyric-like text that statistically mimics Kanye’s vocabulary and phrasing — without copying any specific song.

How it works

The program reads a plain text file containing training data

The text is tokenized into words

A first-order Markov chain is built, mapping each word to the set of words that can follow it, along with their frequencies

New text is generated by randomly sampling next words according to these learned probabilities

This approach captures local stylistic patterns (word choice, repetition, phrasing) but does not understand meaning or context.

Example

Input (training data):

[All Kanye West lyrics combined into a single text corpus]


Generated output:

i feel the love in the night and the lights go down
money on my mind i been working for the crown


The output is statistically similar in style, but entirely generated.

Usage
python markov.py lyrics.txt 100


lyrics.txt — training text file (e.g. combined lyrics)

100 — number of words to generate

Disclaimer

This project is for educational and experimental purposes only.
The generated text is algorithmically produced and does not reproduce copyrighted lyrics verbatim.

Why this project

This project demonstrates:

probabilistic modeling

basic natural language processing concepts

command-line program design

clean functional decomposition in Python

It also serves as a stepping stone toward more advanced language models.
